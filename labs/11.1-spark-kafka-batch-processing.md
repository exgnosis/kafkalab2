<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

# Lab 11.1: Spark Structured Streaming with Kafka

### Overview

Use Spark to process data in Kafka.  This will be batch processing.

### Run time

40 mins

## Step-1: Inspect Spark Services

Spark service should be running.  Inspect it by running `jps`.  You will see a `master` and `worker` process.

```bash
$   jps
```

output:

```console
...
.... master
.... worker
```

## Step-2: Start PySpark

```bash
$   ~/apps/spark/bin/pyspark
```

In the Spark shell, try these commands

```python
> f = spark.read.text('/etc/hosts')
> f.count()
> f.show()
> exit()
```

You might see output like this:

```console
+--------------------+
|               value|
+--------------------+
| 127.0.0.1     localhost|
|::1    localhost ip6...|
|fe00::0        ip6-localnet|
|ff00::0        ip6-mcast...|
|ff02::1        ip6-allnodes|
|ff02::2        ip6-allro...|
|10.10.0.2      2282d19...|
+--------------------+
```

## Step-3: Create Clickstream Topic

If you don't have the topic, create it as follows

```bash
$   ~/apps/kafka/bin/kafka-topics.sh  --bootstrap-server localhost:9092   \
    --create --topic test --replication-factor 1  --partitions 10
```

## Step-4: Install Kafka Python Library

```bash
$   pip install confluent_kafka
```

## Step-5: Generate Clickstream data

Open a terminal and run the python producer:

```bash
$   cd  ~/kafka-labs/python

$   python producer-clickstream.py
```

This will populate clickstream topic

## Step-6: Inspect Spark Consumer Code

Inspect file: `python/spark-consumer-batch.py`

We will work through TODO items in this file

## Step-7: Run Spark Consumer

Open a terminal and execute these commands:

```bash

$   cd  ~/kafka-labs/python

$  ~/apps/spark/bin/spark-submit  --master local[2] \
    --driver-class-path .  \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 \
    spark-consumer-batch.py
```

This will initialize Spark session and connect to Kafka.  You will output like following

```console
root
 |-- key: string (nullable = true)
 |-- value: string (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
```

**ACTION: Observe how Spark reads from Kafka and the data types**

### Troubleshoting Tips

I have noticed in some instances, downloading dependencies will fail.  If that happens, try this:

```bash
# clean up ivy caches
$   rm -rf    ~/.ivy2/cache   ~/.ivy2/jars

# If the above doesn't work, try cleaning up local mvn repository
# $   rm -rf  ~/.m2/repository
```

And then re-run spark application.

References

- [issue #190](https://github.com/databricks/spark-deep-learning/issues/190#issuecomment-479999455)

## Step-8: TODO-1 - Go through all Kafka Data

Here is code process entire Kafka data

```python
# code for TODO-1
print ("total count: ", df.count())
df.show()
```

**ACTION: Fill this in for TODO-1 placehold**

**ACTION: Run the code again  (See Step-4 for run command)**

**ACTION: Observe the output.  You may see output as follows:**

```console
+------------+--------------------+-----------+---------+------+--------------------+
|         key|               value|      topic|partition|offset|           timestamp|
+------------+--------------------+-----------+---------+------+--------------------+
|facebook.com|{"timestamp": 164...|clickstream|        9|  1988|2022-01-22 04:37:...|
|facebook.com|{"timestamp": 164...|clickstream|        9|  1989|2022-01-22 04:37:...|
|facebook.com|{"timestamp": 164...|clickstream|        9|  1990|2022-01-22 04:37:...|
+------------+--------------------+-----------+---------+------+--------------------+
```

## Step-9: TODO-2 - Extract Kafka Data into Dataframes

Now let's extract JSON data into Spark dataframe, so we can process it easily.

Use the code below

```python
# code for TODO-2

# first we need a schema
schema = StructType(
    [
        StructField('timestamp', StringType(), True),
        StructField('ip', StringType(), True),
        StructField('user', StringType(), True),
        StructField('action', StringType(), True),
        StructField('domain', StringType(), True),
        StructField('campaign', StringType(), True),
        StructField('cost', IntegerType(), True)
    ]
)

# extract value from JSON string and populate into another df
df2 = df.withColumn("value", from_json("value", schema))\
    .select(col('key'), col('value.*'))

df2.printSchema()
df2.sample(0.1).show()
```

**ACTION: Fill this in for TODO-2 placehold**

**ACTION: Run the code again  (See Step-4 for run command)**

**ACTION: Observe the output.  You may see output as follows:**

```console
+------------+-------------+---------+-------+-------+------------+-----------+----+
|         key|    timestamp|       ip|   user| action|      domain|   campaign|cost|
+------------+-------------+---------+-------+-------+------------+-----------+----+
|facebook.com|1642826798006|  1.4.4.3|user-33|blocked|facebook.com|campaign-59|  93|
|   gmail.com|1642826796603|  1.2.8.1|user-18| viewed|   gmail.com|campaign-17|  66|
|   gmail.com|1642826796803|  3.1.2.7|user-67| viewed|   gmail.com|campaign-88|  78|
+------------+-------------+---------+-------+-------+------------+-----------+----+
```

## Step-10: TODO-3 : Run a Spark-SQL Query on the Data

we are going to run a SQL query on Kafka data!  How cool is that?

```python
# code for TODO-3
# Let's do a SQL query on Kafka data!
df2.createOrReplaceTempView("clickstream_view")

# calculate avg spend per domain
sql_str = """
SELECT domain, count(*) as impressions, MIN(cost), AVG(cost), MAX(cost)
FROM clickstream_view
GROUP BY domain
"""
spark.sql(sql_str).show()
```

**ACTION: Fill in the code**

**ACTION: run the file**

**ACTION: and observe the results**

```console
+------------+-----------+---------+------------------+---------+
|      domain|impressions|min(cost)|         avg(cost)|max(cost)|
+------------+-----------+---------+------------------+---------+
| youtube.com|       2579|        1|50.037999224505626|      100|
|   gmail.com|       2669|        1|51.293368302735104|      100|
|facebook.com|       2571|        1|50.560093348891485|      100|
| twitter.com|       2663|        1| 50.78708223807735|      100|
|linkedin.com|       2682|        1|51.319910514541384|      100|
+------------+-----------+---------+------------------+---------+
```

## Step-11: TODO-4: Run another Spark SQL query

Let's continue with Spark SQL and do another query.  This time, we are going to calculte the ad performance  - views / clicks / blocks - per domain.

Here is the code:

```python
# code for TODO-4

# count clicks/views/blocks
sql_str = """
SELECT domain, action, COUNT(action)
FROM clickstream_view
GROUP BY ???, ???
ORDER BY ???
"""
spark.sql(sql_str).show()
```

**ACTION: Fill in the code**

**ACTION: run the file**

**ACTION: and observe the results**


```console
+------------+-------+-------------+
|      domain| action|count(action)|
+------------+-------+-------------+
|facebook.com|blocked|          839|
|facebook.com|clicked|          878|
|facebook.com| viewed|          854|
|   gmail.com|blocked|          919|
|   gmail.com|clicked|          876|
|   gmail.com| viewed|          874|
|linkedin.com|blocked|          869|
|linkedin.com|clicked|          951|
|linkedin.com| viewed|          862|
| twitter.com|blocked|          889|
| twitter.com|clicked|          892|
| twitter.com| viewed|          882|
| youtube.com|blocked|          822|
| youtube.com|clicked|          914|
| youtube.com| viewed|          843|
+------------+-------+-------------+
```

## Step-12: Bonus Lab: Tweak the SQL to Produce a Result Table like this

```console
| domain       | views | clicks | blocks |   |
|--------------|-------|--------|--------|---|
| facebook.com | 5     | 7      | 12     |   |
| twitter.com  | 10    | 5      | 9      |   |
| youtube.com  | 15    | 4      | 10     |   |
```
